"""
å‘é‡å­˜å‚¨æœåŠ¡ - RAGæ ¸å¿ƒç»„ä»¶
"""
from typing import List, Dict, Optional
from pathlib import Path
import logging
from datetime import datetime

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    Docx2txtLoader,
    TextLoader
)
from langchain.schema import Document

logger = logging.getLogger(__name__)


class VectorStoreService:
    """å‘é‡æ•°æ®åº“æœåŠ¡ - ä½¿ç”¨ChromaDB"""
    
    _instance = None  # å•ä¾‹æ¨¡å¼
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, persist_directory: str = "./data/vector_db"):
        """åˆå§‹åŒ–å‘é‡åº“"""
        if self._initialized:
            return
        
        self.persist_directory = persist_directory
        Path(persist_directory).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–Embeddings
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"  # æ›´ä¾¿å®œçš„æ¨¡å‹
        )
        
        # åˆå§‹åŒ–å‘é‡åº“
        self.vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings,
            collection_name="documents"
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,        # æ¯å—1000å­—ç¬¦
            chunk_overlap=200,      # é‡å 200å­—ç¬¦
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "!", "?", "ï¼›", " ", ""]
        )
        
        self._initialized = True
        logger.info("âœ… VectorStoreService initialized")
    
    def load_document(self, file_path: str) -> List[Document]:
        """
        æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£
        
        æ”¯æŒæ ¼å¼ï¼š.pdf, .md, .docx, .txt
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ–‡ä»¶ç±»å‹æ˜ å°„
        loaders = {
            '.pdf': PyPDFLoader,
            '.md': UnstructuredMarkdownLoader,
            '.markdown': UnstructuredMarkdownLoader,
            '.docx': Docx2txtLoader,
            '.txt': TextLoader
        }
        
        loader_class = loaders.get(file_path.suffix.lower())
        if not loader_class:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}\n"
                f"æ”¯æŒçš„ç±»å‹: {', '.join(loaders.keys())}"
            )
        
        try:
            loader = loader_class(str(file_path))
            documents = loader.load()
            logger.info(f"ğŸ“„ Loaded {len(documents)} pages from {file_path.name}")
            return documents
        except Exception as e:
            logger.error(f"âŒ Failed to load {file_path}: {e}")
            raise
    
    def ingest_document(
        self,
        file_path: str,
        user_id: str,
        doc_id: str,
        metadata: Optional[Dict] = None
    ) -> Dict:
        """
        æ‘„å–æ–‡æ¡£åˆ°å‘é‡åº“
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            user_id: ç”¨æˆ·ID
            doc_id: æ–‡æ¡£IDï¼ˆç”¨äºåç»­åˆ é™¤ï¼‰
            metadata: é¢å¤–çš„å…ƒæ•°æ®
            
        Returns:
            æ‘„å–ç»“æœå­—å…¸
        """
        try:
            # 1. åŠ è½½æ–‡æ¡£
            documents = self.load_document(file_path)
            
            # 2. åˆ†å—
            chunks = self.text_splitter.split_documents(documents)
            logger.info(f"ğŸ“ Split into {len(chunks)} chunks")
            
            # 3. æ·»åŠ å…ƒæ•°æ®
            base_metadata = {
                'user_id': user_id,
                'doc_id': doc_id,
                'source': str(file_path),
                'file_name': Path(file_path).name,
                'file_type': Path(file_path).suffix,
                'ingested_at': datetime.now().isoformat()
            }
            
            if metadata:
                base_metadata.update(metadata)
            
            for i, chunk in enumerate(chunks):
                chunk.metadata.update(base_metadata)
                chunk.metadata['chunk_id'] = f"{doc_id}_chunk_{i}"
            
            # 4. å‘é‡åŒ–å¹¶å­˜å‚¨
            self.vectorstore.add_documents(chunks)
            
            logger.info(f"âœ… Ingested {Path(file_path).name}: {len(chunks)} chunks")
            
            return {
                'success': True,
                'num_chunks': len(chunks),
                'file_name': Path(file_path).name,
                'doc_id': doc_id
            }
            
        except Exception as e:
            logger.error(f"âŒ Document ingestion failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def search(
        self,
        query: str,
        user_id: str,
        k: int = 3,
        score_threshold: float = 0.5
    ) -> List[Dict]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            user_id: ç”¨æˆ·IDï¼ˆåªè¿”å›è¯¥ç”¨æˆ·çš„æ–‡æ¡£ï¼‰
            k: è¿”å›ç»“æœæ•°é‡
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰
            results = self.vectorstore.similarity_search_with_score(
                query,
                k=k * 2,  # å¤šè·å–ä¸€äº›ï¼Œç„¶åè¿‡æ»¤
                filter={'user_id': user_id}
            )
            
            # è½¬æ¢åˆ†æ•°ï¼ˆChromaDBè¿”å›çš„æ˜¯è·ç¦»ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
            filtered_results = []
            for doc, distance in results:
                # è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼Œè½¬æ¢ä¸º0-1çš„ç›¸ä¼¼åº¦åˆ†æ•°
                similarity = 1 / (1 + distance)
                
                if similarity >= score_threshold:
                    filtered_results.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'score': similarity
                    })
            
            # é™åˆ¶è¿”å›æ•°é‡
            filtered_results = filtered_results[:k]
            
            logger.info(f"ğŸ” Search '{query}': found {len(filtered_results)} results")
            return filtered_results
            
        except Exception as e:
            logger.error(f"âŒ Search failed: {e}")
            return []
    
    def delete_document(self, user_id: str, doc_id: str) -> bool:
        """
        åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡å—
        
        Args:
            user_id: ç”¨æˆ·ID
            doc_id: æ–‡æ¡£ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # ChromaDBçš„deleteæ–¹æ³•éœ€è¦æŒ‡å®šIDs
            # æˆ‘ä»¬é€šè¿‡å…ƒæ•°æ®æŸ¥è¯¢æ‰¾åˆ°æ‰€æœ‰chunk_ids
            results = self.vectorstore.get(
                where={
                    "user_id": user_id,
                    "doc_id": doc_id
                }
            )
            
            if results and 'ids' in results:
                ids_to_delete = results['ids']
                self.vectorstore.delete(ids=ids_to_delete)
                logger.info(f"ğŸ—‘ï¸  Deleted {len(ids_to_delete)} chunks for doc {doc_id}")
                return True
            else:
                logger.warning(f"âš ï¸  No chunks found for doc {doc_id}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Delete failed: {e}")
            return False
    
    def get_stats(self, user_id: str) -> Dict:
        """
        è·å–ç”¨æˆ·çš„å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            # è·å–æ‰€æœ‰è¯¥ç”¨æˆ·çš„æ–‡æ¡£
            results = self.vectorstore.get(
                where={"user_id": user_id}
            )
            
            total_chunks = len(results['ids']) if results and 'ids' in results else 0
            
            # ç»Ÿè®¡æ–‡æ¡£æ•°ï¼ˆé€šè¿‡doc_idå»é‡ï¼‰
            doc_ids = set()
            if results and 'metadatas' in results:
                for metadata in results['metadatas']:
                    if 'doc_id' in metadata:
                        doc_ids.add(metadata['doc_id'])
            
            return {
                'total_documents': len(doc_ids),
                'total_chunks': total_chunks,
                'user_id': user_id
            }
            
        except Exception as e:
            logger.error(f"âŒ Get stats failed: {e}")
            return {
                'total_documents': 0,
                'total_chunks': 0,
                'user_id': user_id
            }


# å•ä¾‹å®ä¾‹ï¼ˆå…¨å±€ä½¿ç”¨ï¼‰
vector_store_service = VectorStoreService()